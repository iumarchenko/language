{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустить seq2seq, seq2seq с внимаием для перевода русских слов + описать наблюдения по качеству <br>\n",
    "Данные в папке data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### без внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 50\n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = 'data/rus-eng/rus.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "125/125 [==============================] - 35s 277ms/step - loss: 1.1225 - accuracy: 0.7737 - val_loss: 0.9152 - val_accuracy: 0.7596\n",
      "Epoch 2/50\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.7344 - accuracy: 0.8025 - val_loss: 0.7773 - val_accuracy: 0.7944\n",
      "Epoch 3/50\n",
      "125/125 [==============================] - 34s 270ms/step - loss: 0.6309 - accuracy: 0.8336 - val_loss: 0.6855 - val_accuracy: 0.8135\n",
      "Epoch 4/50\n",
      "125/125 [==============================] - 34s 274ms/step - loss: 0.5615 - accuracy: 0.8453 - val_loss: 0.6277 - val_accuracy: 0.8244\n",
      "Epoch 5/50\n",
      "125/125 [==============================] - 34s 269ms/step - loss: 0.5224 - accuracy: 0.8531 - val_loss: 0.5948 - val_accuracy: 0.8307\n",
      "Epoch 6/50\n",
      "125/125 [==============================] - 34s 273ms/step - loss: 0.5596 - accuracy: 0.8495 - val_loss: 0.6026 - val_accuracy: 0.8270\n",
      "Epoch 7/50\n",
      "125/125 [==============================] - 33s 263ms/step - loss: 0.5005 - accuracy: 0.8575 - val_loss: 0.5699 - val_accuracy: 0.8371\n",
      "Epoch 8/50\n",
      "125/125 [==============================] - 33s 265ms/step - loss: 0.4795 - accuracy: 0.8621 - val_loss: 0.5537 - val_accuracy: 0.8387\n",
      "Epoch 9/50\n",
      "125/125 [==============================] - 35s 281ms/step - loss: 0.4655 - accuracy: 0.8653 - val_loss: 0.5431 - val_accuracy: 0.8427\n",
      "Epoch 10/50\n",
      "125/125 [==============================] - 37s 297ms/step - loss: 0.4530 - accuracy: 0.8686 - val_loss: 0.5281 - val_accuracy: 0.8461\n",
      "Epoch 11/50\n",
      "125/125 [==============================] - 35s 279ms/step - loss: 0.4418 - accuracy: 0.8710 - val_loss: 0.5204 - val_accuracy: 0.8481\n",
      "Epoch 12/50\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.4302 - accuracy: 0.8742 - val_loss: 0.5099 - val_accuracy: 0.8527\n",
      "Epoch 13/50\n",
      "125/125 [==============================] - 33s 262ms/step - loss: 0.4199 - accuracy: 0.8770 - val_loss: 0.5011 - val_accuracy: 0.8534\n",
      "Epoch 14/50\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.4100 - accuracy: 0.8801 - val_loss: 0.4960 - val_accuracy: 0.8567\n",
      "Epoch 15/50\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.4002 - accuracy: 0.8829 - val_loss: 0.4884 - val_accuracy: 0.8583\n",
      "Epoch 16/50\n",
      "125/125 [==============================] - 33s 263ms/step - loss: 0.3905 - accuracy: 0.8860 - val_loss: 0.4793 - val_accuracy: 0.8616\n",
      "Epoch 17/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.3814 - accuracy: 0.8886 - val_loss: 0.4751 - val_accuracy: 0.8645\n",
      "Epoch 18/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.3722 - accuracy: 0.8917 - val_loss: 0.4717 - val_accuracy: 0.8658\n",
      "Epoch 19/50\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.3635 - accuracy: 0.8941 - val_loss: 0.4647 - val_accuracy: 0.8671\n",
      "Epoch 20/50\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.3552 - accuracy: 0.8965 - val_loss: 0.4599 - val_accuracy: 0.8692\n",
      "Epoch 21/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.3463 - accuracy: 0.8992 - val_loss: 0.4532 - val_accuracy: 0.8708\n",
      "Epoch 22/50\n",
      "125/125 [==============================] - 32s 260ms/step - loss: 0.3378 - accuracy: 0.9018 - val_loss: 0.4522 - val_accuracy: 0.8711\n",
      "Epoch 23/50\n",
      "125/125 [==============================] - 33s 260ms/step - loss: 0.3300 - accuracy: 0.9039 - val_loss: 0.4490 - val_accuracy: 0.8715\n",
      "Epoch 24/50\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.3226 - accuracy: 0.9059 - val_loss: 0.4481 - val_accuracy: 0.8730\n",
      "Epoch 25/50\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.3151 - accuracy: 0.9079 - val_loss: 0.4403 - val_accuracy: 0.8748\n",
      "Epoch 26/50\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.3075 - accuracy: 0.9103 - val_loss: 0.4375 - val_accuracy: 0.8759\n",
      "Epoch 27/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.3002 - accuracy: 0.9123 - val_loss: 0.4384 - val_accuracy: 0.8756\n",
      "Epoch 28/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.2934 - accuracy: 0.9141 - val_loss: 0.4316 - val_accuracy: 0.8779\n",
      "Epoch 29/50\n",
      "125/125 [==============================] - 32s 259ms/step - loss: 0.2851 - accuracy: 0.9163 - val_loss: 0.4362 - val_accuracy: 0.8766\n",
      "Epoch 30/50\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.2785 - accuracy: 0.9181 - val_loss: 0.4313 - val_accuracy: 0.8786\n",
      "Epoch 31/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.2709 - accuracy: 0.9201 - val_loss: 0.4288 - val_accuracy: 0.8798\n",
      "Epoch 32/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.2648 - accuracy: 0.9221 - val_loss: 0.4279 - val_accuracy: 0.8805\n",
      "Epoch 33/50\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.2573 - accuracy: 0.9241 - val_loss: 0.4298 - val_accuracy: 0.8802\n",
      "Epoch 34/50\n",
      "125/125 [==============================] - 32s 259ms/step - loss: 0.2512 - accuracy: 0.9258 - val_loss: 0.4318 - val_accuracy: 0.8808\n",
      "Epoch 35/50\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.2446 - accuracy: 0.9277 - val_loss: 0.4276 - val_accuracy: 0.8818\n",
      "Epoch 36/50\n",
      "125/125 [==============================] - 33s 264ms/step - loss: 0.2398 - accuracy: 0.9288 - val_loss: 0.4274 - val_accuracy: 0.8828\n",
      "Epoch 37/50\n",
      "125/125 [==============================] - 33s 263ms/step - loss: 0.2319 - accuracy: 0.9313 - val_loss: 0.4304 - val_accuracy: 0.8816\n",
      "Epoch 38/50\n",
      "125/125 [==============================] - 33s 262ms/step - loss: 0.2256 - accuracy: 0.9328 - val_loss: 0.4316 - val_accuracy: 0.8826\n",
      "Epoch 39/50\n",
      "125/125 [==============================] - 33s 262ms/step - loss: 0.2198 - accuracy: 0.9345 - val_loss: 0.4306 - val_accuracy: 0.8829\n",
      "Epoch 40/50\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.2135 - accuracy: 0.9365 - val_loss: 0.4306 - val_accuracy: 0.8837\n",
      "Epoch 41/50\n",
      "125/125 [==============================] - 33s 261ms/step - loss: 0.2074 - accuracy: 0.9382 - val_loss: 0.4336 - val_accuracy: 0.8840\n",
      "Epoch 42/50\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.2030 - accuracy: 0.9395 - val_loss: 0.4314 - val_accuracy: 0.8846\n",
      "Epoch 43/50\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.1960 - accuracy: 0.9415 - val_loss: 0.4374 - val_accuracy: 0.8837\n",
      "Epoch 44/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.1913 - accuracy: 0.9429 - val_loss: 0.4383 - val_accuracy: 0.8850\n",
      "Epoch 45/50\n",
      "125/125 [==============================] - 32s 257ms/step - loss: 0.1859 - accuracy: 0.9445 - val_loss: 0.4438 - val_accuracy: 0.8836\n",
      "Epoch 46/50\n",
      "125/125 [==============================] - 32s 256ms/step - loss: 0.1801 - accuracy: 0.9459 - val_loss: 0.4459 - val_accuracy: 0.8839\n",
      "Epoch 47/50\n",
      "125/125 [==============================] - 32s 258ms/step - loss: 0.1746 - accuracy: 0.9477 - val_loss: 0.4516 - val_accuracy: 0.8840\n",
      "Epoch 48/50\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.1700 - accuracy: 0.9492 - val_loss: 0.4493 - val_accuracy: 0.8845\n",
      "Epoch 49/50\n",
      "125/125 [==============================] - 33s 267ms/step - loss: 0.1651 - accuracy: 0.9507 - val_loss: 0.4524 - val_accuracy: 0.8848\n",
      "Epoch 50/50\n",
      "125/125 [==============================] - 32s 255ms/step - loss: 0.1602 - accuracy: 0.9519 - val_loss: 0.4601 - val_accuracy: 0.8837\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Уходи!\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Уходи!\n",
      "\n",
      "-\n",
      "Input sentence: Go.\n",
      "Decoded sentence: Уходи!\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здо.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здо.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здо.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здо.\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: Здо.\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: Бегите!\n",
      "\n",
      "-\n",
      "Input sentence: Who?\n",
      "Decoded sentence: Кто белосо?\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Врт т!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Врт т!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Врт т!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Врт т!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Врт т!\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Decoded sentence: Врт т!\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Отони!\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Decoded sentence: Отони!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Смот!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Смот!\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Decoded sentence: Смот!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump!\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Decoded sentence: Прыгайте!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Остановитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Остановитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Decoded sentence: Остановитесь!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Ждите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Ждите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Ждите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: Ждите!\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Wait.\n",
      "Decoded sentence: Подождите.\n",
      "\n",
      "-\n",
      "Input sentence: Do it.\n",
      "Decoded sentence: Сделай это.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Продолжай.\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Decoded sentence: Продолжай.\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Дратат!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Дратат!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Дратат!\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: Дратат!\n",
      "\n",
      "-\n",
      "Input sentence: Hurry!\n",
      "Decoded sentence: Поспошите.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побежал.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побежал.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побежал.\n",
      "\n",
      "-\n",
      "Input sentence: I ran.\n",
      "Decoded sentence: Я побежал.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я видую.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я видую.\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Decoded sentence: Я видую.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я подерала.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я подерала.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: Я подерала.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я был на пакажали?\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я был на пакажали?\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я был на пакажали?\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: Я был на пакажали?\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: Он тед!\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Постоть насте.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Постоть насте.\n",
      "\n",
      "-\n",
      "Input sentence: Relax.\n",
      "Decoded sentence: Постоть насте.\n",
      "\n",
      "-\n",
      "Input sentence: Shoot!\n",
      "Decoded sentence: Присерете!\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Smile.\n",
      "Decoded sentence: Улыбнитесь.\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Decoded sentence: Начанать!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: За ваше здоровье!\n",
      "\n",
      "-\n",
      "Input sentence: Eat it.\n",
      "Decoded sentence: Поезжайте это.\n",
      "\n",
      "-\n",
      "Input sentence: Eat up.\n",
      "Decoded sentence: Подежай са не с б  мом.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни смонь его.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни смонь его.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни смонь его.\n",
      "\n",
      "-\n",
      "Input sentence: Freeze!\n",
      "Decoded sentence: Ни смонь его.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Вставай.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Вставай.\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Decoded sentence: Вставай.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Decoded sentence: Идите сейчас.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Decoded sentence: Понял!\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Понял?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Понял?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Понял?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Decoded sentence: Понял?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c вниманием"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/rus-eng/rus.txt'\n",
    "num_samples = 10000\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Zа-яА-Я?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(preprocess_sentence(input_text))\n",
    "    target_texts.append(preprocess_sentence(target_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, inp_lang_tokenizer = tokenize(input_texts)\n",
    "target_tensor, targ_lang_tokenizer = tokenize(target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "    \n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.6834\n",
      "Epoch 2 Loss 1.2709\n",
      "Epoch 3 Loss 1.0974\n",
      "Epoch 4 Loss 0.9577\n",
      "Epoch 5 Loss 0.8320\n",
      "Epoch 6 Loss 0.7154\n",
      "Epoch 7 Loss 0.6107\n",
      "Epoch 8 Loss 0.5175\n",
      "Epoch 9 Loss 0.4331\n",
      "Epoch 10 Loss 0.3617\n",
      "Epoch 11 Loss 0.3058\n",
      "Epoch 12 Loss 0.2636\n",
      "Epoch 13 Loss 0.2343\n",
      "Epoch 14 Loss 0.2104\n",
      "Epoch 15 Loss 0.1932\n",
      "Epoch 16 Loss 0.1846\n",
      "Epoch 17 Loss 0.1724\n",
      "Epoch 18 Loss 0.1640\n",
      "Epoch 19 Loss 0.1593\n",
      "Epoch 20 Loss 0.1533\n",
      "Epoch 21 Loss 0.1487\n",
      "Epoch 22 Loss 0.1454\n",
      "Epoch 23 Loss 0.1427\n",
      "Epoch 24 Loss 0.1404\n",
      "Epoch 25 Loss 0.1386\n",
      "Epoch 26 Loss 0.1370\n",
      "Epoch 27 Loss 0.1340\n",
      "Epoch 28 Loss 0.1319\n",
      "Epoch 29 Loss 0.1294\n",
      "Epoch 30 Loss 0.1273\n",
      "Epoch 31 Loss 0.1258\n",
      "Epoch 32 Loss 0.1258\n",
      "Epoch 33 Loss 0.1246\n",
      "Epoch 34 Loss 0.1243\n",
      "Epoch 35 Loss 0.1229\n",
      "Epoch 36 Loss 0.1224\n",
      "Epoch 37 Loss 0.1208\n",
      "Epoch 38 Loss 0.1199\n",
      "Epoch 39 Loss 0.1182\n",
      "Epoch 40 Loss 0.1171\n",
      "Epoch 41 Loss 0.1157\n",
      "Epoch 42 Loss 0.1152\n",
      "Epoch 43 Loss 0.1159\n",
      "Epoch 44 Loss 0.1158\n",
      "Epoch 45 Loss 0.1199\n",
      "Epoch 46 Loss 0.1196\n",
      "Epoch 47 Loss 0.1181\n",
      "Epoch 48 Loss 0.1144\n",
      "Epoch 49 Loss 0.1120\n",
      "Epoch 50 Loss 0.1097\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    fontdict = {'fontsize': 14}\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    #ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    #ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()\n",
    "    \n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marchenko-i-u\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\pylab.py:161: UserWarning: pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hello how are you <end>\n",
      "Predicted translation: как ты . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'hello how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### без внимания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "попробуем запустить аналогичный код без внимания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "    \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        #self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        #context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        attention_weights = 0\n",
    "        x = self.embedding(x)\n",
    "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.lstm(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index)+1\n",
    "vocab_tar_size = len(targ_lang_tokenizer.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder_11/embedding_22/embeddings:0', 'encoder_11/gru_22/gru_cell_22/kernel:0', 'encoder_11/gru_22/gru_cell_22/recurrent_kernel:0', 'encoder_11/gru_22/gru_cell_22/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['encoder_11/embedding_22/embeddings:0', 'encoder_11/gru_22/gru_cell_22/kernel:0', 'encoder_11/gru_22/gru_cell_22/recurrent_kernel:0', 'encoder_11/gru_22/gru_cell_22/bias:0'] when minimizing the loss.\n",
      "Epoch 1 Loss 1.9355\n",
      "Epoch 2 Loss 1.3746\n",
      "Epoch 3 Loss 1.2993\n",
      "Epoch 4 Loss 1.2368\n",
      "Epoch 5 Loss 1.1804\n",
      "Epoch 6 Loss 1.1347\n",
      "Epoch 7 Loss 1.0973\n",
      "Epoch 8 Loss 1.0645\n",
      "Epoch 9 Loss 1.0375\n",
      "Epoch 10 Loss 1.0153\n",
      "Epoch 11 Loss 0.9991\n",
      "Epoch 12 Loss 0.9837\n",
      "Epoch 13 Loss 0.9733\n",
      "Epoch 14 Loss 0.9654\n",
      "Epoch 15 Loss 0.9584\n",
      "Epoch 16 Loss 0.9544\n",
      "Epoch 17 Loss 0.9500\n",
      "Epoch 18 Loss 0.9470\n",
      "Epoch 19 Loss 0.9440\n",
      "Epoch 20 Loss 0.9417\n",
      "Epoch 21 Loss 0.9401\n",
      "Epoch 22 Loss 0.9385\n",
      "Epoch 23 Loss 0.9370\n",
      "Epoch 24 Loss 0.9364\n",
      "Epoch 25 Loss 0.9339\n",
      "Epoch 26 Loss 0.9333\n",
      "Epoch 27 Loss 0.9324\n",
      "Epoch 28 Loss 0.9314\n",
      "Epoch 29 Loss 0.9306\n",
      "Epoch 30 Loss 0.9294\n",
      "Epoch 31 Loss 0.9289\n",
      "Epoch 32 Loss 0.9283\n",
      "Epoch 33 Loss 0.9278\n",
      "Epoch 34 Loss 0.9270\n",
      "Epoch 35 Loss 0.9267\n",
      "Epoch 36 Loss 0.9258\n",
      "Epoch 37 Loss 0.9253\n",
      "Epoch 38 Loss 0.9248\n",
      "Epoch 39 Loss 0.9244\n",
      "Epoch 40 Loss 0.9237\n",
      "Epoch 41 Loss 0.9236\n",
      "Epoch 42 Loss 0.9225\n",
      "Epoch 43 Loss 0.9225\n",
      "Epoch 44 Loss 0.9222\n",
      "Epoch 45 Loss 0.9217\n",
      "Epoch 46 Loss 0.9211\n",
      "Epoch 47 Loss 0.9208\n",
      "Epoch 48 Loss 0.9207\n",
      "Epoch 49 Loss 0.9202\n",
      "Epoch 50 Loss 0.9194\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Input: <start> hello how are you <end>\n",
      "Predicted translation: я не могу это . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "translate(u'hello how are you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод - метод с вниманием работает лучше. это видно и по переводу и по Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
